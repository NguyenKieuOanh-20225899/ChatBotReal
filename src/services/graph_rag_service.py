# File: src/services/graph_rag_service.py
import os
import json
import time
from typing import Tuple, List, Dict

from langchain_groq import ChatGroq
from langchain_google_genai import GoogleGenerativeAIEmbeddings
from langchain_community.vectorstores import FAISS
from dotenv import load_dotenv

class GraphRAGService:
    def __init__(self, vector_db_path: str = "data/artifacts", graph_path: str = "data/knowledge_graph.json"):
        load_dotenv()

        self.google_api_key = os.getenv("GOOGLE_API_KEY")
        self.groq_api_key = os.getenv("GROQ_API_KEY")

        if not self.groq_api_key:
            raise ValueError("‚ùå Thi·∫øu GROQ_API_KEY trong file .env")

        # 1. KH·ªûI T·∫†O LLM
        print("‚ö° ƒêang k·∫øt n·ªëi t·ªõi Groq (Llama-3.1-8b-instant)...")
        self.llm = ChatGroq(
            temperature=0.1,
            model_name="llama-3.1-8b-instant",
            api_key=self.groq_api_key,
            max_retries=2
        )

        # 2. LOAD VECTOR DB (S·ª≠a l·ªói quan tr·ªçng ·ªü ƒë√¢y)
        print(f"üì¶ Loading Vector Database t·ª´: {vector_db_path}")
        self.embeddings = GoogleGenerativeAIEmbeddings(
            model="models/text-embedding-004",
            google_api_key=self.google_api_key
        )
        try:
            # L∆ØU √ù: Th√™m index_name="faiss" ƒë·ªÉ kh·ªõp v·ªõi file faiss.faiss ƒë√£ t·∫°o
            self.vector_db = FAISS.load_local(
                vector_db_path,
                self.embeddings,
                allow_dangerous_deserialization=True,
                index_name="faiss"  # <--- QUAN TR·ªåNG: Ph·∫£i kh·ªõp v·ªõi l√∫c save
            )
            print("‚úÖ Vector DB loaded th√†nh c√¥ng.")
        except Exception as e:
            print(f"‚ö†Ô∏è Kh√¥ng load ƒë∆∞·ª£c Vector DB: {e}")
            print("üëâ G·ª£i √Ω: H√£y ch·∫°y 'python scripts/run_pipeline.py' ƒë·ªÉ t·∫°o d·ªØ li·ªáu tr∆∞·ªõc.")
            self.vector_db = None

        # 3. LOAD KNOWLEDGE GRAPH
        print("üï∏Ô∏è Loading Knowledge Graph...")
        self.graph_nodes = {}
        self.graph_edges = []
        try:
            with open(graph_path, "r", encoding="utf-8") as f:
                data = json.load(f)
                for node in data.get("nodes", []):
                    self.graph_nodes[node["id"]] = node
                self.graph_edges = data.get("edges", [])
            print(f"‚úÖ Graph loaded: {len(self.graph_nodes)} nodes, {len(self.graph_edges)} edges.")
        except Exception as e:
            print(f"‚ö†Ô∏è Kh√¥ng load ƒë∆∞·ª£c Graph JSON: {e}")

    def _find_related_nodes(self, initial_nodes: List[str]) -> List[Dict]:
        """T√¨m c√°c node li√™n quan (b∆∞·ªõc nh·∫£y 1)"""
        related_info = []
        for edge in self.graph_edges:
            source = edge["from"]
            target = edge["to"]
            relation = edge["relation"]

            if source in initial_nodes:
                target_node = self.graph_nodes.get(target)
                if target_node:
                    topic = target_node.get("topic", "")
                    # L·∫•y th√™m ngu·ªìn n·∫øu c√≥
                    src_doc = target_node.get("sources", [])
                    src_str = f" (Ngu·ªìn: {src_doc[0]})" if src_doc else ""
                    related_info.append(f"- {source} {relation} {target}: {topic}{src_str}")

        return related_info[:10]

    def query(self, query_text: str, k: int = 4) -> Tuple[str, dict, float]:
        t0 = time.perf_counter()

        # B∆Ø·ªöC 1: VECTOR SEARCH
        context_parts = []
        found_articles = set()
        vec_sources = []

        if self.vector_db:
            hits = self.vector_db.similarity_search(query_text, k=k)
            for h in hits:
                content = h.page_content
                context_parts.append(content)
                vec_sources.append(h.metadata.get("source", "Unknown"))

                # T√¨m ID ƒëi·ªÅu lu·∫≠t trong n·ªôi dung t√¨m ƒë∆∞·ª£c
                for node_id in self.graph_nodes:
                    # T√¨m ƒë∆°n gi·∫£n: n·∫øu "ƒêi·ªÅu 5" c√≥ trong text
                    if node_id in content:
                        found_articles.add(node_id)

        # B∆Ø·ªöC 2: GRAPH SEARCH
        graph_context = []
        if found_articles:
            graph_context = self._find_related_nodes(list(found_articles))

        # B∆Ø·ªöC 3: T·∫†O PROMPT
        vector_str = "\n\n".join(context_parts)
        graph_str = "\n".join(graph_context) if graph_context else "Kh√¥ng t√¨m th·∫•y m·ªëi li√™n h·ªá m·ªü r·ªông."

        prompt = f"""
B·∫°n l√† Tr·ª£ l√Ω Lu·∫≠t s∆∞ AI. Tr·∫£ l·ªùi c√¢u h·ªèi d·ª±a tr√™n th√¥ng tin sau:

[TH√îNG TIN VƒÇN B·∫¢N - VECTOR]:
{vector_str}

[LI√äN K·∫æT PH√ÅP L√ù - GRAPH]:
{graph_str}

[C√ÇU H·ªéI]: {query_text}

Y√äU C·∫¶U:
1. Tr·∫£ l·ªùi ng·∫Øn g·ªçn, ch√≠nh x√°c.
2. Tr√≠ch d·∫´n ƒëi·ªÅu lu·∫≠t (V√≠ d·ª•: Theo ƒêi·ªÅu 5...).
3. N·∫øu Graph cung c·∫•p th√¥ng tin li√™n quan, h√£y b·ªï sung.

TR·∫¢ L·ªúI:
"""
        try:
            response = self.llm.invoke(prompt)
            answer = response.content
        except Exception as e:
            answer = f"L·ªói AI: {e}"

        latency = time.perf_counter() - t0

        meta = {
            "vector_sources": vec_sources,
            "graph_edges_used": len(graph_context)
        }

        return answer, meta, latency

    def close(self):
        pass
