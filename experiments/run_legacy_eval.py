import json
import os
import sys
from tqdm import tqdm
from dotenv import load_dotenv
# Import cÃ¡c thÆ° viá»‡n cÅ© báº¡n dÃ¹ng
from langchain_community.vectorstores import FAISS
from langchain_google_genai import GoogleGenerativeAIEmbeddings

# Import hÃ m tÃ­nh Ä‘iá»ƒm
sys.path.append(os.path.abspath(os.path.join(os.path.dirname(__file__), '..')))
from experiments.eval_metrics import recall_at_k, mrr, ndcg_at_k

# 1. Cáº¥u hÃ¬nh
load_dotenv()
API_KEY = os.getenv("GOOGLE_API_KEY")
BASE_DIR = os.path.dirname(os.path.dirname(os.path.abspath(__file__)))
VECTOR_DIR = os.path.join(BASE_DIR, "data", "vector_db")
DEVSET_PATH = os.path.join(BASE_DIR, "experiments", "devset", "dev_100.jsonl")

def normalize_id(doc_id):
    """
    Chuáº©n hÃ³a ID siÃªu máº¡nh: Cáº¯t bá» má»i háº­u tá»‘ rÆ°á»m rÃ  Ä‘á»ƒ láº¥y tÃªn vÄƒn báº£n gá»‘c.
    VÃ­ dá»¥ input:
      - "VanBanGoc_52.2014.QH13_knowledge_Äiá»u 103"
      - "VanBanGoc_52.2014.QH13_chunks#12"
      - "117:2024:NÄ-CP_clean.txt"
    Output chung: "VanBanGoc_52.2014.QH13" hoáº·c "117_2024_NÄ-CP"
    """
    if not doc_id: return ""
    s = str(doc_id)

    # 1. Láº¥y tÃªn file (bá» Ä‘Æ°á»ng dáº«n thÆ° má»¥c)
    s = os.path.basename(s)

    # 2. Thay tháº¿ cÃ¡c kÃ½ tá»± Ä‘áº·c biá»‡t gÃ¢y lá»—i
    s = s.replace(":", "_") # Sá»­a lá»—i 117:2024

    # 3. Bá» Ä‘uÃ´i file
    for ext in [".json", ".txt", "_clean"]:
        s = s.replace(ext, "")

    # 4. Cáº®T Bá» Háº¬U Tá» (Quan trá»ng nháº¥t)
    # Cáº¯t ngay khi gáº·p cÃ¡c tá»« khÃ³a nÃ y
    keywords_to_cut = ["_knowledge", "_chunks", "_Äiá»u", ":Äiá»u"]
    for kw in keywords_to_cut:
        if kw in s:
            s = s.split(kw)[0]

    return s.strip()

def main():
    print(f"ğŸ”„ Äang táº£i Vector DB cÅ© tá»«: {VECTOR_DIR}")
    try:
        embedding = GoogleGenerativeAIEmbeddings(model="models/text-embedding-004", google_api_key=API_KEY)
        vector_db = FAISS.load_local(VECTOR_DIR, embedding, allow_dangerous_deserialization=True)
    except Exception as e:
        print(f"âŒ Lá»—i: KhÃ´ng tÃ¬m tháº¥y hoáº·c khÃ´ng load Ä‘Æ°á»£c Vector DB cÅ©.\n{e}")
        return

    print(f"ğŸ“– Äang Ä‘á»c bá»™ dá»¯ liá»‡u kiá»ƒm tra...")
    dev_data = [json.loads(l) for l in open(DEVSET_PATH, "r", encoding="utf-8")]

    R10, MRR, N10 = [], [], []

    print("ğŸš€ Báº¯t Ä‘áº§u Ä‘Ã¡nh giÃ¡...")
    debug_count = 0

    for ex in tqdm(dev_data):
        query = ex["question"]
        gold_ids = [normalize_id(g) for g in ex["gold"]]

        # TÃ¬m kiáº¿m top 10
        docs = vector_db.similarity_search(query, k=10)

        # Láº¥y ID tá»« metadata (Æ°u tiÃªn 'source' hoáº·c 'file_name')
        raw_retrieved = [d.metadata.get("source") or d.metadata.get("file_name") or "NO_SOURCE" for d in docs]
        retrieved_ids = [normalize_id(r) for r in raw_retrieved]

        # --- Debug láº¡i Ä‘á»ƒ cháº¯c cháº¯n ID Ä‘Ã£ khá»›p ---
        if debug_count < 3:
            print(f"\n--- DEBUG Query {debug_count+1} ---")
            print(f"CÃ¢u há»i: {query}")
            print(f"Gold (Chuáº©n):   {gold_ids}")
            print(f"Retr (TÃ¬m Ä‘c):  {retrieved_ids}")
            debug_count += 1
        # ---------------------------------------

        R10.append(recall_at_k(retrieved_ids, gold_ids, k=10))
        MRR.append(mrr(retrieved_ids, gold_ids))
        N10.append(ndcg_at_k(retrieved_ids, gold_ids, k=10))

    print("\nğŸ“Š === Káº¾T QUáº¢ ÄÃNH GIÃ CHATBOT CÅ¨ (VECTOR ONLY) ===")
    print(f"Recall@10 = {sum(R10)/len(R10):.3f}")
    print(f"MRR       = {sum(MRR)/len(MRR):.3f}")
    print(f"nDCG@10   = {sum(N10)/len(N10):.3f}")

if __name__ == "__main__":
    main()