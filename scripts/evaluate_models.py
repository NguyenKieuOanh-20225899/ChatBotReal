# File: scripts/evaluate_models.py
import sys
import os
import time
import json
import pandas as pd
import yaml
from dotenv import load_dotenv
from langchain_groq import ChatGroq

# Setup Ä‘Æ°á»ng dáº«n
sys.path.append(os.path.abspath(os.path.join(os.path.dirname(__file__), '..')))

# Import Service
from src.core.search_engine import HybridSearcher
try:
    from src.services.graph_rag_service import GraphRAGService
except ImportError:
    GraphRAGService = None

load_dotenv()
GROQ_API_KEY = os.getenv("GROQ_API_KEY")

# Khá»Ÿi táº¡o AI Judge
judge_llm = ChatGroq(
    api_key=GROQ_API_KEY,
    model_name="llama-3.1-8b-instant",
    temperature=0 # Nhiá»‡t Ä‘á»™ 0 Ä‘á»ƒ cháº¥m Ä‘iá»ƒm nháº¥t quÃ¡n
)

def load_config():
    with open("config/config.yaml", "r", encoding="utf-8") as f:
        return yaml.safe_load(f)

# --- HÃ€M CHáº¤M ÄIá»‚M ---
def ai_grade(question, ground_truth, model_answer, mode="essay"):
    """
    HÃ m cháº¥m Ä‘iá»ƒm Ä‘a nÄƒng.
    """
    if not model_answer:
        return 0, "KhÃ´ng tráº£ lá»i"

    if mode == "mcq":
        # Prompt cháº¥m Tráº¯c nghiá»‡m
        prompt = f"""
        Báº¡n lÃ  mÃ¡y cháº¥m thi tráº¯c nghiá»‡m.
        [CÃ‚U Há»I]: {question}
        [ÄÃP ÃN ÄÃšNG]: {ground_truth}
        [CÃ‚U TRáº¢ Lá»œI Cá»¦A AI]: {model_answer}

        YÃŠU Cáº¦U:
        1. Kiá»ƒm tra xem AI cÃ³ chá»n Ä‘Ãºng Ä‘Ã¡p Ã¡n ({ground_truth}) khÃ´ng.
        2. Náº¿u Ä‘Ãºng -> score: 1, Náº¿u sai -> score: 0.

        OUTPUT JSON: {{"score": 1, "reason": "Chá»n Ä‘Ãºng B"}}
        """
    else:
        # Prompt cháº¥m Tá»± luáº­n
        prompt = f"""
        Báº¡n lÃ  GiÃ¡m kháº£o Luáº­t.
        [CÃ‚U Há»I]: {question}
        [ÄÃP ÃN CHUáº¨N]: {ground_truth}
        [TRáº¢ Lá»œI Cá»¦A AI]: {model_answer}

        YÃŠU Cáº¦U:
        1. Cháº¥m Ä‘iá»ƒm Ä‘á»™ chÃ­nh xÃ¡c ngá»¯ nghÄ©a (Thang 0-10).
        2. KhÃ´ng cáº§n Ä‘Ãºng tá»«ng chá»¯, chá»‰ cáº§n Ä‘Ãºng Ã½ phÃ¡p lÃ½.

        OUTPUT JSON: {{"score": 8.5, "reason": "Äá»§ Ã½ nhÆ°ng thiáº¿u trÃ­ch dáº«n"}}
        """

    try:
        res = judge_llm.invoke(prompt)
        content = res.content.strip()
        # Parse JSON
        start = content.find('{')
        end = content.rfind('}') + 1
        result = json.loads(content[start:end])
        return result.get("score", 0), result.get("reason", "")
    except:
        return 0, "Lá»—i cháº¥m Ä‘iá»ƒm"

# --- HÃ€M CHáº Y ÄÃNH GIÃ (DÃ¹ng chung) ---
def run_evaluation(test_file, output_file, mode, graph_service):
    if not os.path.exists(test_file):
        print(f"âš ï¸ KhÃ´ng tÃ¬m tháº¥y file: {test_file}. Bá» qua.")
        return

    print(f"\nâš¡ Äang Ä‘Ã¡nh giÃ¡: {mode.upper()} (File: {test_file})...")

    with open(test_file, "r", encoding="utf-8") as f:
        test_cases = json.load(f)

    results = []
    total_score = 0

    for idx, case in enumerate(test_cases):
        q = case["question"]
        gt = case["ground_truth"]

        print(f"   ğŸ”¹ CÃ¢u {idx+1}: {q[:50]}...")

        # Query Graph RAG
        ans = "N/A"
        sources = 0
        latency = 0

        if graph_service:
            try:
                # Náº¿u lÃ  tráº¯c nghiá»‡m, nháº¯c AI chá»n A,B,C,D
                query_input = q
                if mode == "mcq":
                    query_input += "\n(Chá»‰ chá»n 1 Ä‘Ã¡p Ã¡n Ä‘Ãºng nháº¥t A, B, C hoáº·c D vÃ  giáº£i thÃ­ch ngáº¯n gá»n)"

                ans, meta, latency = graph_service.query(query_input)
                sources = len(meta.get("vector_sources", [])) + meta.get("graph_edges_used", 0)
            except Exception as e:
                ans = f"Error: {e}"

        # Cháº¥m Ä‘iá»ƒm
        score, reason = ai_grade(q, gt, ans, mode)
        total_score += score

        # LÆ°u káº¿t quáº£
        results.append({
            "CÃ¢u há»i": q,
            "ÄÃ¡p Ã¡n chuáº©n": gt,
            "AI Tráº£ lá»i": ans,
            "Äiá»ƒm": score,
            "LÃ½ do": reason,
            "Nguá»“n tÃ¬m tháº¥y": sources,
            "Thá»i gian (s)": round(latency, 2)
        })

    # Xuáº¥t Excel
    df = pd.DataFrame(results)
    df.to_excel(output_file, index=False)

    # In bÃ¡o cÃ¡o nhanh
    print(f"   âœ… ÄÃ£ xong! Káº¿t quáº£ lÆ°u táº¡i: {output_file}")
    if mode == "mcq":
        # Tráº¯c nghiá»‡m tÃ­nh theo % Ä‘Ãºng
        accuracy = (total_score / len(test_cases)) * 100
        print(f"   ğŸ“Š Äá»™ chÃ­nh xÃ¡c (Accuracy): {accuracy:.2f}% ({int(total_score)}/{len(test_cases)} cÃ¢u Ä‘Ãºng)")
    else:
        # Tá»± luáº­n tÃ­nh Ä‘iá»ƒm trung bÃ¬nh
        avg_score = total_score / len(test_cases)
        print(f"   ğŸ“Š Äiá»ƒm cháº¥t lÆ°á»£ng TB: {avg_score:.2f}/10")

def main():
    print("ğŸš€ Báº®T Äáº¦U QUÃ TRÃŒNH ÄÃNH GIÃ TÃCH BIá»†T")

    # Init Graph Service
    graph_service = GraphRAGService() if GraphRAGService else None

    if not graph_service:
        print("âŒ Lá»—i: KhÃ´ng khá»Ÿi táº¡o Ä‘Æ°á»£c GraphRAGService.")
        return

    # 1. ÄÃ¡nh giÃ¡ Tráº¯c nghiá»‡m (MCQ)
    run_evaluation(
        test_file="data/test_set_mcq.json",
        output_file="ket_qua_trac_nghiem.xlsx",
        mode="mcq",
        graph_service=graph_service
    )

    # 2. ÄÃ¡nh giÃ¡ Tá»± luáº­n (Essay)
    run_evaluation(
        test_file="data/test_set_essay.json",
        output_file="ket_qua_tu_luan.xlsx",
        mode="essay",
        graph_service=graph_service
    )

    print("\nğŸ‰ HOÃ€N Táº¤T TOÃ€N Bá»˜!")

if __name__ == "__main__":
    main()
