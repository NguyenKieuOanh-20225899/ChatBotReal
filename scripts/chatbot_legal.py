import os
import sys
import time
import re
from dotenv import load_dotenv
import google.generativeai as genai

# áº¨n thÃ´ng bÃ¡o log khÃ´ng cáº§n thiáº¿t cá»§a gRPC vÃ  TensorFlow
os.environ['GRPC_VERBOSITY'] = 'ERROR'
os.environ['TF_CPP_MIN_LOG_LEVEL'] = '3'

# 1. Setup Ä‘Æ°á»ng dáº«n tuyá»‡t Ä‘á»‘i Ä‘á»ƒ trÃ¡nh lá»—i File Not Found
# Láº¥y Ä‘Æ°á»ng dáº«n Ä‘áº¿n thÆ° má»¥c gá»‘c cá»§a dá»± Ã¡n
BASE_DIR = os.path.dirname(os.path.dirname(os.path.abspath(__file__)))
sys.path.append(BASE_DIR)

# Import bá»™ tÃ¬m kiáº¿m nÃ¢ng cao (Hybrid + Rerank) tá»« dá»± Ã¡n
try:
    from src.retrieval_service import LegalRetriever
except ImportError as e:
    print(f"âŒ Lá»—i: KhÃ´ng thá»ƒ tÃ¬m tháº¥y thÆ° má»¥c 'src'. HÃ£y Ä‘áº£m báº£o báº¡n Ä‘ang cháº¡y tá»« thÆ° má»¥c gá»‘c cá»§a dá»± Ã¡n. Chi tiáº¿t: {e}")
    sys.exit(1)

# 2. Cáº¥u hÃ¬nh API
load_dotenv()
API_KEY = os.getenv("GOOGLE_API_KEY")

if not API_KEY:
    print("âŒ Lá»—i: ChÆ°a tÃ¬m tháº¥y GOOGLE_API_KEY trong file .env")
    print("ğŸ‘‰ HÃ£y láº¥y API Key miá»…n phÃ­ táº¡i: https://aistudio.google.com/")
    sys.exit(1)

genai.configure(api_key=API_KEY)

# 3. Khá»Ÿi táº¡o há»‡ thá»‘ng (Chá»‰ cháº¡y 1 láº§n)
print("â³ Äang khá»Ÿi táº¡o há»‡ thá»‘ng tÃ¬m kiáº¿m (Hybrid Search + Reranker)...")
CONFIG_PATH = os.path.join(BASE_DIR, "experiments", "config.yaml")

try:
    # Khá»Ÿi táº¡o bá»™ truy xuáº¥t dá»¯ liá»‡u
    retriever = LegalRetriever(config_path=CONFIG_PATH)

    # --- KHáº®C PHá»¤C Lá»–I 404: Tá»± Ä‘á»™ng tÃ¬m tÃªn mÃ´ hÃ¬nh kháº£ dá»¥ng ---
    print("ğŸ” Äang xÃ¡c thá»±c mÃ´ hÃ¬nh Gemini 1.5 Flash...")
    models = [m.name for m in genai.list_models() if 'generateContent' in m.supported_generation_methods]
    
    # Æ¯u tiÃªn cÃ¡c phiÃªn báº£n cá»§a gemini-1.5-flash Ä‘á»ƒ dÃ¹ng Free Tier
    target_model_name = None
    for m in ["models/gemini-1.5-flash", "models/gemini-1.5-flash-latest", "models/gemini-1.5-flash-001"]:
        if m in models:
            target_model_name = m
            break
            
    if not target_model_name:
        # Náº¿u khÃ´ng tháº¥y báº£n Flash, láº¥y Ä‘áº¡i diá»‡n Ä‘áº§u tiÃªn cÃ³ trong danh sÃ¡ch
        target_model_name = next((m for m in models if "flash" in m), models[0])

    print(f"âœ… Äang sá»­ dá»¥ng mÃ´ hÃ¬nh: {target_model_name}")
    model = genai.GenerativeModel(model_name=target_model_name)
    print("âœ… Há»‡ thá»‘ng Ä‘Ã£ sáºµn sÃ ng!\n")

except Exception as e:
    print(f"âŒ Lá»—i khá»Ÿi táº¡o há»‡ thá»‘ng: {e}")
    sys.exit(1)

def extract_source_from_text(text):
    """LÃ m sáº¡ch tÃªn nguá»“n: loáº¡i bá» pháº§n má»Ÿ rá»™ng file vÃ  format láº¡i cho Ä‘áº¹p"""
    match = re.match(r"\[(.*?)]:", text)
    if match:
        source_name = match.group(1)
        # XÃ³a cÃ¡c háº­u tá»‘ file Ä‘á»ƒ hiá»ƒn thá»‹ ngáº¯n gá»n
        source_name = re.sub(r"(_knowledge\.json|_clean\.txt|\.txt)$", "", source_name)
        return source_name
    return "KhÃ´ng rÃµ nguá»“n"

def query_advanced(query: str):
    """
    Quy trÃ¬nh RAG nÃ¢ng cao:
    1. Retrieve (Hybrid Search)
    2. Rerank (Cross-Encoder)
    3. Generate (Gemini 1.5 Flash)
    """
    t0 = time.perf_counter()

    try:
        # BÆ¯á»šC 1 & 2: TRUY XUáº¤T VÃ€ Xáº¾P Háº NG Láº I
        context_list = retriever.retrieve(query)

        if not context_list:
            return ("TÃ´i khÃ´ng tÃ¬m tháº¥y vÄƒn báº£n phÃ¡p luáº­t nÃ o liÃªn quan Ä‘áº¿n cÃ¢u há»i nÃ y trong cÆ¡ sá»Ÿ dá»¯ liá»‡u.", 
                    [], time.perf_counter() - t0)

        # Láº¥y danh sÃ¡ch nguá»“n duy nháº¥t (Ä‘Ã£ lÃ m sáº¡ch tÃªn)
        unique_sources = sorted(list(set(extract_source_from_text(c) for c in context_list)))
        context_text = "\n\n".join(context_list)

        # BÆ¯á»šC 3: XÃ‚Y Dá»°NG PROMPT
        prompt = f"""
Báº¡n lÃ  trá»£ lÃ½ luáº­t sÆ° AI am hiá»ƒu sÃ¢u sáº¯c phÃ¡p luáº­t Viá»‡t Nam.
Nhiá»‡m vá»¥: Tráº£ lá»i cÃ¢u há»i dá»±a trÃªn CÃC ÄOáº N VÄ‚N Báº¢N ÄÆ¯á»¢C CUNG Cáº¤P dÆ°á»›i Ä‘Ã¢y.

YÃªu cáº§u:
1. Tráº£ lá»i chÃ­nh xÃ¡c, dá»©t khoÃ¡t. 
2. Báº®T BUá»˜C trÃ­ch dáº«n Äiá»u luáº­t cá»¥ thá»ƒ (VÃ­ dá»¥: Theo Äiá»u 5...).
3. Náº¿u thÃ´ng tin khÃ´ng náº±m trong dá»¯ liá»‡u cung cáº¥p, hÃ£y nÃ³i "TÃ´i khÃ´ng tÃ¬m tháº¥y quy Ä‘á»‹nh nÃ y trong cÆ¡ sá»Ÿ dá»¯ liá»‡u hiá»‡n táº¡i".
4. Giá»ng vÄƒn trang trá»ng, khÃ¡ch quan.

--- Dá»® LIá»†U THAM KHáº¢O ---
{context_text}
--- Káº¾T THÃšC Dá»® LIá»†U ---

CÃ¢u há»i: {query}
CÃ¢u tráº£ lá»i:
"""

        # BÆ¯á»šC 4: Gá»ŒI GEMINI SINH PHáº¢N Há»’I
        response = model.generate_content(prompt)
        answer = response.text

    except Exception as e:
        answer = f"âš ï¸ Lá»—i xá»­ lÃ½ yÃªu cáº§u: {str(e)}"
        unique_sources = []

    latency = time.perf_counter() - t0
    return answer, unique_sources, latency

if __name__ == "__main__":
    print("ğŸ¤– CHATBOT PHÃP LUáº¬T (RAG + Gemini 1.5 Flash)")
    print("---------------------------------------------")
    
    while True:
        user_query = input("\nğŸ‘¤ Nháº­p cÃ¢u há»i (gÃµ 'exit' Ä‘á»ƒ thoÃ¡t): ").strip()
        
        if user_query.lower() in ["exit", "quit", "thoat", "t"]:
            print("ğŸ‘‹ Cáº£m Æ¡n báº¡n Ä‘Ã£ sá»­ dá»¥ng há»‡ thá»‘ng!")
            break

        if not user_query:
            continue

        print("ğŸ” Äang phÃ¢n tÃ­ch dá»¯ liá»‡u phÃ¡p luáº­t...")
        ans, srcs, t_elapsed = query_advanced(user_query)

        print("\nğŸ¤– Tráº£ lá»i:")
        print(ans)

        if srcs:
            print("\nğŸ“š Nguá»“n trÃ­ch dáº«n:")
            for s in srcs:
                print(f"  â†’ {s}")

        print(f"\nâš¡ Thá»i gian xá»­ lÃ½: {t_elapsed:.2f}s")