# File: scripts/create_vector_index.py
import os
import json
import pickle
import numpy as np
from tqdm import tqdm
from dotenv import load_dotenv

# Import cÃ¡c thÆ° viá»‡n AI
from langchain_community.vectorstores import FAISS
from langchain_google_genai import GoogleGenerativeAIEmbeddings
from rank_bm25 import BM25Okapi

# Cáº¥u hÃ¬nh Ä‘Æ°á»ng dáº«n
BASE_DIR = os.path.dirname(os.path.dirname(os.path.abspath(__file__)))
CHUNK_DIR = os.path.join(BASE_DIR, "data", "chunks")
ARTIFACTS_DIR = os.path.join(BASE_DIR, "data", "artifacts")

# Táº¡o thÆ° má»¥c artifacts náº¿u chÆ°a cÃ³
os.makedirs(ARTIFACTS_DIR, exist_ok=True)

load_dotenv()
google_api_key = os.getenv("GOOGLE_API_KEY")

if not google_api_key:
    print("âŒ Lá»—i: ChÆ°a cÃ³ GOOGLE_API_KEY trong file .env")
    exit(1)

def tokenize_vn(text):
    """
    HÃ m tÃ¡ch tá»« Ä‘Æ¡n giáº£n cho tiáº¿ng Viá»‡t Ä‘á»ƒ cháº¡y BM25.
    (TÃ¡ch theo khoáº£ng tráº¯ng vÃ  lowercase)
    """
    return text.lower().split()

def main():
    print("ğŸš€ Báº¯t Ä‘áº§u táº¡o Index cho Hybrid Search (Vector + BM25)...")
    print(f"   - Embeddings: Google (text-embedding-004)")
    print(f"   - Keyword: BM25Okapi")

    # 1. Äá»c dá»¯ liá»‡u tá»« Chunks
    docs = []   # LÆ°u ná»™i dung text
    metas = []  # LÆ°u metadata (tÃªn file, nguá»“n...)

    if not os.path.exists(CHUNK_DIR):
        print(f"âŒ KhÃ´ng tÃ¬m tháº¥y thÆ° má»¥c {CHUNK_DIR}. HÃ£y cháº¡y split_text.py trÆ°á»›c.")
        exit(1)

    files = [f for f in os.listdir(CHUNK_DIR) if f.endswith(".json")]
    if not files:
        print("âŒ ThÆ° má»¥c chunks rá»—ng!")
        exit(1)

    print("ğŸ“¦ Äang táº£i dá»¯ liá»‡u chunks...")
    for filename in tqdm(files):
        path = os.path.join(CHUNK_DIR, filename)
        try:
            with open(path, "r", encoding="utf-8") as f:
                chunks = json.load(f)

            for chunk in chunks:
                # Xá»­ lÃ½ tÆ°Æ¡ng thÃ­ch cáº£ format cÅ© (str) vÃ  má»›i (dict)
                if isinstance(chunk, dict):
                    text = chunk.get("page_content", "")
                    meta = chunk.get("metadata", {})
                    # Náº¿u metadata chÆ°a cÃ³ source, láº¥y tá»« tÃªn file
                    if "source" not in meta:
                        meta["source"] = filename.replace("_chunks.json", ".pdf")
                else:
                    text = str(chunk)
                    meta = {"source": filename.replace("_chunks.json", ".pdf")}

                if text.strip(): # Chá»‰ láº¥y Ä‘oáº¡n cÃ³ ná»™i dung
                    docs.append(text)
                    metas.append(meta)
        except Exception as e:
            print(f"âš ï¸ Lá»—i Ä‘á»c file {filename}: {e}")

    print(f"âœ… ÄÃ£ táº£i {len(docs)} Ä‘oáº¡n vÄƒn báº£n.")

    # 2. Táº¡o & LÆ°u BM25 (Cho Keyword Search)
    print("ğŸ”  Äang táº¡o chá»‰ má»¥c BM25...")
    tokenized_docs = [tokenize_vn(doc) for doc in tqdm(docs, desc="Tokenizing")]
    bm25 = BM25Okapi(tokenized_docs)

    with open(os.path.join(ARTIFACTS_DIR, "bm25.pkl"), "wb") as f:
        pickle.dump(bm25, f)
    print("   -> ÄÃ£ lÆ°u data/artifacts/bm25.pkl")

    # 3. LÆ°u Docs & Metas (Quan trá»ng cho HybridSearcher)
    print("ğŸ’¾ Äang lÆ°u docs.json vÃ  metas.json...")
    with open(os.path.join(ARTIFACTS_DIR, "docs.json"), "w", encoding="utf-8") as f:
        json.dump(docs, f, ensure_ascii=False)

    with open(os.path.join(ARTIFACTS_DIR, "metas.json"), "w", encoding="utf-8") as f:
        json.dump(metas, f, ensure_ascii=False)

    # 4. Táº¡o & LÆ°u FAISS (Cho Semantic Search)
    print("ğŸ§  Äang táº¡o Vector Index (FAISS)...")
    embeddings = GoogleGenerativeAIEmbeddings(
        model="models/text-embedding-004",
        google_api_key=google_api_key
    )

    # Táº¡o vector store
    vector_db = FAISS.from_texts(docs, embeddings, metadatas=metas)

    # LÆ°u index FAISS vÃ o artifacts
    vector_db.save_local(ARTIFACTS_DIR, index_name="faiss")
    print(f"   -> ÄÃ£ lÆ°u FAISS index vÃ o {ARTIFACTS_DIR}")

    print("\nğŸ‰ HOÃ€N Táº¤T! Dá»¯ liá»‡u Ä‘Ã£ sáºµn sÃ ng cho Hybrid Search.")

if __name__ == "__main__":
    main()
